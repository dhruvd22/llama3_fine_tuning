# Configuration for LoRA fine-tuning
model:
  # Path to the base model weights (local path or HF repo)
  base_model_path: /workspace/data/mygitstuff/updated_models/base_model_r001
  hf_token: null  # Optional Hugging Face token

training:
  datasets:
    - /workspace/data/mygitstuff/llama3_fine_tuning/datasets/F02_preprocessed_SINGLETABLE.jsonl
    - /workspace/data/mygitstuff/llama3_fine_tuning/datasets/F02_preprocessed_JOINS.jsonl
  output_dir: /workspace/data/mygitstuff/checkpoints/lora_run02testv3
  num_epochs: 3
  # Reduce the per-device batch size to avoid GPU OOM errors
  batch_size: 2
  learning_rate: 4e-4
  logging_steps: 10
  save_steps: 100
  # When false, sequences are padded dynamically to the longest in each batch
  pad_to_max_length: false
  lora_r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  # Accumulate gradients to keep an effective batch size similar to the
  # original configuration without overloading GPU memory
  gradient_accumulation_steps: 8

wandb:
  enabled: true
  project: llama-finetune
  run_name: lora-training
