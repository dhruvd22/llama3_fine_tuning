# Example Axolotl configuration for LoRA fine-tuning
model:
  base_model_path: models/my-llama  # Path to the downloaded base model

# Training dataset paths (update with your own data)
dataset:
  train: datasets/train.jsonl
  val: datasets/val.jsonl

# Output directory to store adapters and checkpoints
output_dir: checkpoints/lora-training

# LoRA adapter configuration
adapter: lora
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05

# General training parameters
sequence_len: 4096
micro_batch_size: 1
gradient_accumulation_steps: 8
num_epochs: 3
save_steps: 500
logging_steps: 10

# Weights & Biases integration
wandb_project: llama3-finetuning
wandb_api_key: ${WANDB_API_KEY}

# Hugging Face token can also be provided via HF_TOKEN env var
hf_token: ${HF_TOKEN}
